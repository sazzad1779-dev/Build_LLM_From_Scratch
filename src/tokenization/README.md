# ğŸ§  Tokenization Metrics Guide

### Understanding Fertility, CPT, WFR, and Normalization (Before Training SentencePiece)

When building a tokenizer for a custom dataset (especially for LLMs), training SentencePiece directly without evaluation can lead to inefficient tokenization.
This guide explains the **core metrics** that help evaluate tokenizer quality **before and after training**.

---

## ğŸ“Œ Why These Metrics Matter

A tokenizer affects:

* Model context length usage
* Training speed
* Memory efficiency
* Language understanding quality

Bad tokenization = longer sequences + slower models + weaker generalization.

---

# 1ï¸âƒ£ Fertility

### ğŸ”¹ Definition

**Fertility** measures how many tokens are produced per word.

[
\textbf{Fertility} = \frac{N_{tokens}}{N_{words}}
]

### Where:

* (N_{tokens}) = total tokens after tokenization
* (N_{words}) = total words in the corpus

### ğŸ”¹ Intuition

How *fragmented* your words become.

### ğŸ”¹ Example

Sentence:

```
à¦†à¦®à¦¿ à¦¤à§‹à¦®à¦¾à¦•à§‡ à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¿
```

| Tokenization                                  | Tokens | Fertility |
| --------------------------------------------- | ------ | --------- |
| ["à¦†à¦®à¦¿","à¦¤à§‹à¦®à¦¾à¦•à§‡","à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¿"]                   | 3      | 1.0 âœ…     |
| ["à¦†","à¦®à¦¿","à¦¤à§‹","à¦®à¦¾","à¦•à§‡","à¦­à¦¾","à¦²à§‹","à¦¬à¦¾","à¦¸à¦¿"] | 9      | 3.0 âŒ     |

### ğŸ”¹ Interpretation

| Fertility | Meaning                   |
| --------- | ------------------------- |
| ~1.0      | Word-level                |
| 1.2â€“2.0   | Good subword tokenization |
| >2.5      | Over-fragmentation        |

---

# 2ï¸âƒ£ CPT â€” Characters Per Token

### ğŸ”¹ Definition

Average number of characters each token represents.

[
\textbf{CPT} = \frac{N_{char}}{N_{tokens}}
]

### Where:

* (N_{char}) = total characters in corpus
* (N_{tokens}) = total tokens

### ğŸ”¹ Intuition

Measures **token information density**.

### ğŸ”¹ Example

Text: `à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶`

| Tokenization             | Tokens | CPT   |
| ------------------------ | ------ | ----- |
| ["à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶"]             | 1      | 7.0 âœ… |
| ["à¦¬à¦¾","à¦‚","à¦²à¦¾","à¦¦à§‡","à¦¶"] | 5      | 1.4 âŒ |

### ğŸ”¹ Interpretation

| CPT       | Meaning                       |
| --------- | ----------------------------- |
| ~1        | Character-level (inefficient) |
| 3.5â€“6     | Ideal subword tokenizer âœ…     |
| Very high | Over-merged tokens            |

---

# 3ï¸âƒ£ WFR â€” Word Fragmentation Rate

### ğŸ”¹ Definition

Fraction of words split into **two or more tokens**.

[
\textbf{WFR} = \frac{N_{split_words}}{N_{words}}
]

### Where:

* (N_{split_words}) = words producing â‰¥2 tokens
* (N_{words}) = total words

### ğŸ”¹ Example

Sentence:

```
internationalization is hard
```

| Word                 | Tokens | Split? |
| -------------------- | ------ | ------ |
| internationalization | 3      | âœ…      |
| is                   | 1      | âŒ      |
| hard                 | 1      | âŒ      |

[
WFR = \frac{1}{3} = 0.33
]

### ğŸ”¹ Interpretation

| WFR     | Meaning                 |
| ------- | ----------------------- |
| 0       | No words split          |
| 0.2â€“0.4 | Normal subword behavior |
| >0.6    | Too fragmented âŒ        |

---

# 4ï¸âƒ£ Normalization

### ğŸ”¹ Definition

A preprocessing transformation applied before tokenization.

[
\boxed{T' = f(T)}
]

Where:

* (T) = original text
* (f) = normalization function
* (T') = normalized text

### ğŸ”¹ Purpose

Ensure **text consistency** and reduce vocabulary duplication.

### ğŸ”¹ Common Normalization Functions

| Operation                | Effect                       |
| ------------------------ | ---------------------------- |
| Unicode NFC/NFKC         | Standardizes character forms |
| Lowercasing              | A â†’ a                        |
| Whitespace normalization | Multiple spaces â†’ one        |
| Quote normalization      | â€œ â€ â†’ "                      |
| Digit normalization      | à§§à§¨à§© â†’ 123 (optional)         |

### ğŸ”¹ Example

Raw:

```
à¦†à¦®à¦¿   à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¿!!!
```

Normalized:

```
à¦†à¦®à¦¿ à¦­à¦¾à¦²à§‹à¦¬à¦¾à¦¸à¦¿ !
```

---

# ğŸ”— How These Metrics Work Together

| Metric        | Measures                 | Goal          |
| ------------- | ------------------------ | ------------- |
| Fertility     | Tokens per word          | Keep low      |
| CPT           | Info per token           | Moderateâ€“high |
| WFR           | Word splitting frequency | Controlled    |
| Normalization | Text consistency         | Reduce noise  |

---

# ğŸ¯ Ideal Tokenizer Characteristics

âœ” Low fertility
âœ” CPT between 3.5â€“6
âœ” WFR below 0.4
âœ” Proper Unicode normalization
âœ” No excessive character-level splitting

---

# ğŸš€ Why This Matters for LLMs

Better tokenization leads to:

* Shorter input sequences
* Faster training
* Better context utilization
* Lower memory use
* Improved generalization

