# üß† Tokenization Metrics Guide

### Understanding Fertility, CPT, WFR, and Normalization (Before Training SentencePiece)

When building a tokenizer for a custom dataset (especially for LLMs), training SentencePiece directly without evaluation can lead to inefficient tokenization.
This guide explains the **core metrics** that help evaluate tokenizer quality **before and after training**.

---

## üìå Why These Metrics Matter

A tokenizer affects:

* Model context length usage
* Training speed
* Memory efficiency
* Language understanding quality

Bad tokenization = longer sequences + slower models + weaker generalization.

---

# 1Ô∏è‚É£ Fertility

### üîπ Definition

**Fertility** measures how many tokens are produced per word.

[
\textbf{Fertility} = \frac{N_{tokens}}{N_{words}}
]

### Where:

* (N_{tokens}) = total tokens after tokenization
* (N_{words}) = total words in the corpus

### üîπ Intuition

How *fragmented* your words become.

### üîπ Example

Sentence:

```
‡¶Ü‡¶Æ‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø
```

| Tokenization                                  | Tokens | Fertility |
| --------------------------------------------- | ------ | --------- |
| ["‡¶Ü‡¶Æ‡¶ø","‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá","‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø"]                   | 3      | 1.0 ‚úÖ     |
| ["‡¶Ü","‡¶Æ‡¶ø","‡¶§‡ßã","‡¶Æ‡¶æ","‡¶ï‡ßá","‡¶≠‡¶æ","‡¶≤‡ßã","‡¶¨‡¶æ","‡¶∏‡¶ø"] | 9      | 3.0 ‚ùå     |

### üîπ Interpretation

| Fertility | Meaning                   |
| --------- | ------------------------- |
| ~1.0      | Word-level                |
| 1.2‚Äì2.0   | Good subword tokenization |
| >2.5      | Over-fragmentation        |

---

# 2Ô∏è‚É£ CPT ‚Äî Characters Per Token

### üîπ Definition

Average number of characters each token represents.

[
\textbf{CPT} = \frac{N_{char}}{N_{tokens}}
]

### Where:

* (N_{char}) = total characters in corpus
* (N_{tokens}) = total tokens

### üîπ Intuition

Measures **token information density**.

### üîπ Example

Text: `‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂`

| Tokenization             | Tokens | CPT   |
| ------------------------ | ------ | ----- |
| ["‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂"]             | 1      | 7.0 ‚úÖ |
| ["‡¶¨‡¶æ","‡¶Ç","‡¶≤‡¶æ","‡¶¶‡ßá","‡¶∂"] | 5      | 1.4 ‚ùå |

### üîπ Interpretation

| CPT       | Meaning                       |
| --------- | ----------------------------- |
| ~1        | Character-level (inefficient) |
| 3.5‚Äì6     | Ideal subword tokenizer ‚úÖ     |
| Very high | Over-merged tokens            |

---

# 3Ô∏è‚É£ WFR ‚Äî Word Fragmentation Rate

### üîπ Definition

Fraction of words split into **two or more tokens**.

[
\textbf{WFR} = \frac{N_{split_words}}{N_{words}}
]

### Where:

* (N_{split_words}) = words producing ‚â•2 tokens
* (N_{words}) = total words

### üîπ Example

Sentence:

```
internationalization is hard
```

| Word                 | Tokens | Split? |
| -------------------- | ------ | ------ |
| internationalization | 3      | ‚úÖ      |
| is                   | 1      | ‚ùå      |
| hard                 | 1      | ‚ùå      |

[
WFR = \frac{1}{3} = 0.33
]

### üîπ Interpretation

| WFR     | Meaning                 |
| ------- | ----------------------- |
| 0       | No words split          |
| 0.2‚Äì0.4 | Normal subword behavior |
| >0.6    | Too fragmented ‚ùå        |

---

# 4Ô∏è‚É£ Normalization

### üîπ Definition

A preprocessing transformation applied before tokenization.

[
\boxed{T' = f(T)}
]

Where:

* (T) = original text
* (f) = normalization function
* (T') = normalized text

### üîπ Purpose

Ensure **text consistency** and reduce vocabulary duplication.

### üîπ Common Normalization Functions

| Operation                | Effect                       |
| ------------------------ | ---------------------------- |
| Unicode NFC/NFKC         | Standardizes character forms |
| Lowercasing              | A ‚Üí a                        |
| Whitespace normalization | Multiple spaces ‚Üí one        |
| Quote normalization      | ‚Äú ‚Äù ‚Üí "                      |
| Digit normalization      | ‡ßß‡ß®‡ß© ‚Üí 123 (optional)         |

### üîπ Example

Raw:

```
‡¶Ü‡¶Æ‡¶ø   ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø!!!
```

Normalized:

```
‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø !
```

---

# üîó How These Metrics Work Together

| Metric        | Measures                 | Goal          |
| ------------- | ------------------------ | ------------- |
| Fertility     | Tokens per word          | Keep low      |
| CPT           | Info per token           | Moderate‚Äìhigh |
| WFR           | Word splitting frequency | Controlled    |
| Normalization | Text consistency         | Reduce noise  |

---

# üéØ Ideal Tokenizer Characteristics

‚úî Low fertility
‚úî CPT between 3.5‚Äì6
‚úî WFR below 0.4
‚úî Proper Unicode normalization
‚úî No excessive character-level splitting

---

# üöÄ Why This Matters for LLMs

Better tokenization leads to:

* Shorter input sequences
* Faster training
* Better context utilization
* Lower memory use
* Improved generalization


---

# üõ† Tokenizer Training with Custom Data

Follow these steps to train a custom SentencePiece tokenizer on your own dataset.

---

## 1Ô∏è‚É£ Clone the repository

```bash
git clone <your-repo-url>
cd <repo>
```

---

## 2Ô∏è‚É£ Sync package requirements

If you are using **uv**:

```bash
uv sync
```

---

## 3Ô∏è‚É£ Run the training script

```bash
uv run -m src.tokenization.train \
  --input_dir "data/dataset_files" \
  --corpus "data/corpus.txt" \
  --vocab_size 128000 \
  --model_save_dir "tokenizer_models1" \
  --model_type "bpe"
```

> This will:
>
> * Merge and normalize text files from `data/dataset_files`
> * Train a SentencePiece tokenizer with BPE model
> * Save the model and vocab in `tokenizer_models1/`

---

## 4Ô∏è‚É£ Training Options

| Option                                      | Description                                         |
| ------------------------------------------- | --------------------------------------------------- |
| `-h, --help`                                | Show help message                                   |
| `--input_dir INPUT_DIR`                     | Directory containing text files                     |
| `--corpus CORPUS`                           | Path to merged training corpus file                 |
| `--model_save_dir MODEL_SAVE_DIR`           | Directory to save the trained model                 |
| `--model_prefix MODEL_PREFIX`               | Base name for model and vocab files                 |
| `--vocab_size VOCAB_SIZE`                   | Vocabulary size (e.g., 16000, 32000, 128000)        |
| `--model_type {unigram,bpe,word,char}`      | SentencePiece model type                            |
| `--character_coverage CHARACTER_COVERAGE`   | Fraction of characters to include (default: 0.9995) |
| `--norm_rule NORM_RULE`                     | Normalization rule (default: nmt_nfkc)              |
| `--sample_size SAMPLE_SIZE`                 | Number of sentences sampled for training            |
| `--max_sentence_length MAX_SENTENCE_LENGTH` | Max sentence length                                 |
| `--byte_fallback`                           | Enable byte fallback for unseen characters          |

---

‚úÖ **Tip:** Adjust `vocab_size` and `model_type` based on your dataset size and language. After training, you can immediately evaluate the tokenizer using the evaluation script to check metrics like **Fertility, CPT, and WFR**.


