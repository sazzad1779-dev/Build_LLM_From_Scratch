# Build_LLM_From_Scratch

This project demonstrates the **complete process of building a Large Language Model (LLM) from scratch**.

---

## Project Overview

In this repository, we go step by step to:

* Prepare and preprocess datasets
* Train a **custom tokenizer**
* Build Architecture
* Pre-train and evaluation
* Finetune and evaluation
* Deployment setup

---

### ðŸ”¹ Prepare and Process Datasets
##### Tokenization Dataset Preparation


##### Pretraining Dataset Preparation

### ðŸ”¹ Tokenization

The first part of this project focuses on **tokenization** and creating a **custom tokenizer** for your dataset.

* You will learn how to:

  * Prepare a corpus from TXT, CSV, or Markdown files
  * Train a SentencePiece tokenizer (BPE, Unigram, etc.)
  * Evaluate tokenizer quality using metrics like **Fertility, CPT, and WFR**

* For detailed instructions, see:
  [**Tokenization Guide**](src/tokenization/README.md)

---

### ðŸ”¹ Next Steps

After tokenization, the project will cover:

1. Building the LLM architecture
2. Training the model on the prepared dataset
3. Evaluating performance

---

## ðŸ”¹ Repository Structure

```
src/
â”œâ”€ preprocessing/        # Scripts for corpus preparation
â”œâ”€ tokenization/        # Scripts for tokenization corpus preparation, tokenizer training, and evaluation
â”œâ”€ modeling/            # Model architecture 
â”œâ”€ pretraining/            # Model pre-training scripts
â”œâ”€ finetuning/             # Model finetuning scripts
â”œâ”€ evaluation/             # Model evaluation scripts
tokenizer_models/        # Trained tokenizer models
README.md               # Root README
```

---

âœ… This root README gives an **overview** and links to the detailed tokenization instructions.

